{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化平行集群并在每个cpu中导入可能用到的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client,Reference\n",
    "rc = Client()\n",
    "rc.ids\n",
    "dv = rc[:]\n",
    "with dv.sync_imports():\n",
    "    import os\n",
    "    import json\n",
    "    from math import log, sqrt\n",
    "    import re\n",
    "    import csv\n",
    "    import sys \n",
    "    import difflib\n",
    "    import numpy as np\n",
    "    from copy import deepcopy\n",
    "    from scipy import sparse\n",
    "    import pandas as pd\n",
    "    from itertools import islice   #导入迭代器\n",
    "    from collections import Counter\n",
    "    import networkx as nx\n",
    "    import pickle as pkl\n",
    "    sys.path.append(\"/data1/cufe/students/2017210761wsy/code\")\n",
    "    import LDA_for_label\n",
    "    import network_bulit\n",
    "    from gensim import corpora\n",
    "    from scipy.sparse import bsr_matrix, dok_matrix\n",
    "    from gensim.models import word2vec\n",
    "    from network_bulit import combine_tuple, trans_title_to_num\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在串行CPU中导入可能用到的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import log, sqrt\n",
    "import re\n",
    "import csv\n",
    "import sys \n",
    "import difflib\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from itertools import islice   #导入迭代器\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "sys.path.append(\"/data1/cufe/students/2017210761wsy/code\")\n",
    "import LDA_for_label\n",
    "import network_bulit\n",
    "from gensim import corpora\n",
    "from scipy.sparse import bsr_matrix, dok_matrix\n",
    "from gensim.models import word2vec\n",
    "from network_bulit import combine_tuple, trans_title_to_num\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 串行中用到的三个小函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取pkl\n",
    "def read_pkl(path_pkl):\n",
    "    x = open(path_pkl, 'rb')\n",
    "    journals_dict = pkl.load(x,encoding='iso-8859-1')\n",
    "    x.close()\n",
    "    return journals_dict\n",
    "#合并并行结果\n",
    "def add_dv(dv_matrix):\n",
    "    if isinstance(dv_matrix[0],pd.core.frame.DataFrame):\n",
    "        result = pd.DataFrame(index = dv_matrix[0].index.tolist(), columns = dv_matrix[0].columns.tolist()).fillna(0)   \n",
    "        for i in range(len(dv_matrix)):\n",
    "            result = result + dv_matrix[i]\n",
    "    elif isinstance(dv_matrix[0],pd.core.frame.DataFrame):\n",
    "        result = {}\n",
    "        for i in range(len(dv_matrix)):\n",
    "            result.update(dv_matrix[i])       \n",
    "    return result\n",
    "#开启并行函数num为线程数\n",
    "def lan_para(num=4):\n",
    "    from ipyparallel import Client,Reference\n",
    "    rc = Client()\n",
    "    rc.ids\n",
    "    dv = rc[:num]\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拓扑相似度家族函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##拓扑相似度\n",
    "def Topology_similarity(graph,subnet_name,nod_pair_list):\n",
    "    similarity_matrix = Topology_similarity_of_distance(graph,subnet_name)/2 +  Topology_similarity_of_com(graph,subnet_name)/2              \n",
    "    return similarity_matrix\n",
    "\n",
    "##依距离的\n",
    "def Topology_similarity_of_distance(graph,subnet_name):\n",
    "    ##距离相似度\n",
    "    print(subnet_name+'\\'s topological distance similarity starts to be constructed')\n",
    "    short_distance = nx.all_pairs_shortest_path(graph)                            #自身的节点距离为1\n",
    "    df_empty = pd.DataFrame(index=graph.nodes(), columns=graph.nodes()).fillna(0)  \n",
    "    for index, row in short_distance:\n",
    "        for link_nod, route in row.items():\n",
    "            df_empty[index][link_nod]  = 1/(2**(len(route)-1))\n",
    "    df_empty_T = deepcopy(df_empty.T) \n",
    "    df_dis_sim = df_empty_T + df_empty                                            #没有连接的就为零\n",
    "    print(subnet_name+'\\'s topological distance similarity has been finished')\n",
    "    df_dis_sim = df_dis_sim.apply(lambda x: x/x.sum(),axis=1)\n",
    "    return  df_dis_sim\n",
    "\n",
    "##依共同引用的\n",
    "def Topology_similarity_of_com(graph,subnet_name,nod_pair_list):\n",
    "    ####################################\n",
    "    def t_com_similarity(nod_pair):   \n",
    "        index1 = nod_pair[0]\n",
    "        index2 = nod_pair[1]\n",
    "        global paper_group_dataframe,df_com_sim\n",
    "        #############################\n",
    "        def single_list(arr, target):\n",
    "            return arr.count(target)\n",
    "        #############################\n",
    "        ##共同引用\n",
    "        sum_cite = single_list(list(paper_group_dataframe.loc[index1]),1)+single_list(list(paper_group_dataframe.loc[index2]),1)\n",
    "        com_cite = single_list(list(paper_group_dataframe.loc[index1]+paper_group_dataframe.loc[index2]),2)\n",
    "        if sum_cite ==0:\n",
    "            copq = 0\n",
    "        else:\n",
    "            copq = com_cite/(sum_cite-com_cite)\n",
    "        ##共同被引用\n",
    "        sum_cited = single_list(list(paper_group_dataframe.loc[:,index1]),1)+single_list(list(paper_group_dataframe.loc[:,index2]),1)\n",
    "        com_cited = single_list(list(paper_group_dataframe.loc[:,index1]+paper_group_dataframe.loc[:,index2]),2)\n",
    "        if sum_cited == 0:\n",
    "            capq = 0\n",
    "        else:\n",
    "            capq = com_cited/(sum_cited-com_cited)\n",
    "        df_com_sim.loc[index1,index2] = copq+capq\n",
    "        df_com_sim.loc[index2,index1] = copq+capq     \n",
    "    ####################################\n",
    "    ##共同引用与共同被引用相似度\n",
    "    print(subnet_name+'\\'s topological citation similarity starts to be constructed')\n",
    "    paper_group_matrix = nx.adjacency_matrix(graph).todense()\n",
    "    paper_group_dataframe = pd.DataFrame(paper_group_matrix, index=graph.nodes(), columns=graph.nodes())\n",
    "    df_com_sim = pd.DataFrame(index=graph.nodes(), columns=graph.nodes()).fillna(0)\n",
    "    ##开启并行\n",
    "    dv = lan_para()\n",
    "    dv['paper_group_dataframe'] = paper_group_dataframe\n",
    "    dv['df_com_sim'] = df_com_sim\n",
    "    dv.map_sync(t_com_similarity, nod_pair_list)\n",
    "    ##填充\n",
    "    df_com_sim =  add_dv(dv['df_com_sim'])\n",
    "    df_com_sim = df_com_sim.apply(lambda x: x/x.sum(),axis=1)\n",
    "    print(subnet_name+'\\'s topological citation similarity has been finished')\n",
    "    return df_com_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 属性相似度家族函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#属性相似度\n",
    "def Attribute_similarity(paper_group, graph, nod_pair_list, path_dictionary, lda_model_path, w2v_model_path, subnet_name,num_topics=4):  \n",
    "    \n",
    "    df_publisher_sim = publisher(paper_group, graph, nod_pair_list, subnet_name)\n",
    "    df_lda_sim = abstract(paper_group, graph, nod_pair_list, path_dictionary,lda_model_path, subnet_name, num_topics =4)\n",
    "    df_key_sim = keywords_and_plus(paper_group, graph, nod_pair_list, w2v_model_path, subnet_name)\n",
    "    \n",
    "    return (df_publisher_sim+df_lda_sim+df_key_sim)/3\n",
    "\n",
    "#发行商\n",
    "def publisher(paper_group, graph, nod_pair_list, subnet_name):\n",
    "    ##################################\n",
    "    def pub_similarity(nod_pair):\n",
    "        index1 = nod_pair[0]\n",
    "        index2 = nod_pair[1]\n",
    "        global paper_group,df_publisher_sim        \n",
    "        if paper_group[index1]['ego_attribute']['publisher'] == paper_group[index2]['ego_attribute']['publisher']:\n",
    "            df_publisher_sim.loc[index1,index2] = 1\n",
    "            df_publisher_sim.loc[index2,index1] = 1\n",
    "    ##################################    \n",
    "    print(subnet_name+'\\'s attribute publisher similarity starts to be constructed')        \n",
    "    df_publisher_sim = pd.DataFrame(index = graph.nodes(), columns = graph.nodes()).fillna(0)\n",
    "    ##开启并行\n",
    "    dv = lan_para()\n",
    "    dv['paper_group'] = paper_group\n",
    "    dv['df_publisher_sim'] = df_publisher_sim\n",
    "    dv.map_sync(pub_similarity, nod_pair_list)\n",
    "    ##填充+归一化\n",
    "    df_publisher_sim =  add_dv(dv['df_publisher_sim'])\n",
    "    df_publisher_sim = df_publisher_sim.apply(lambda x: x/x.sum(),axis=1)       \n",
    "    print(subnet_name+'\\'s attribute publisher similarity has been finished')\n",
    "    return df_publisher_sim\n",
    "\n",
    "##摘要    \n",
    "def abstract(paper_group, graph, nod_pair_list, path_dictionary,lda_model_path, subnet_name, num_topics =4):\n",
    "    ########################################\n",
    "    def abs_similarity(nod_pair):\n",
    "        index1 = nod_pair[0]\n",
    "        index2 = nod_pair[1]\n",
    "        global df_lda_score,df_lda_sim\n",
    "        M = (df_lda_score.loc[index1]+df_lda_score.loc[index2])/2\n",
    "        D1 = (df_lda_score.loc[index1]*((df_lda_score.loc[index1]/M).apply(lambda x:log(x)))).sum(axis=0)\n",
    "        D2 = (df_lda_score.loc[index2]*((df_lda_score.loc[index2]/M).apply(lambda x:log(x)))).sum(axis=0) \n",
    "        topic = 1- sqrt(D1 + D2)\n",
    "        df_lda_sim.loc[index1,index2] = topic\n",
    "        df_lda_sim.loc[index2,index1] = topic         \n",
    "    ########################################\n",
    "    print(subnet_name+'\\'s attribute abstract similarity starts to be constructed')                \n",
    "    ##载入LDA主题模型和字典\n",
    "    dictionary = corpora.Dictionary.load(path_dictionary)\n",
    "    ldamodel = read_pkl(lda_model_path)\n",
    "    ##生成LDA结果储存矩阵\n",
    "    df_lda_score = pd.DataFrame(index = graph.nodes(), columns = [\"lda\"+str(i) for i in range(num_topics)]).fillna(0.00001) #防止除零无意义\n",
    "    for paper, item in paper_group.items():\n",
    "        bow_vector = dictionary.doc2bow(convert_doc_to_wordlist(item['ego_attribute']['abstract']))\n",
    "        for index, score in sorted(ldamodel[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "            df_lda_score.loc[paper,\"lda\"+str(index)] = score+0.00001                                                        #防止除零无意义\n",
    "        print(paper)\n",
    "    #Jensen-shannon散度\n",
    "    df_lda_sim = pd.DataFrame(index = graph.nodes(), columns = graph.nodes()).fillna(0) \n",
    "    ##开启并行\n",
    "    dv = lan_para()\n",
    "    dv['df_lda_score'] = df_lda_score\n",
    "    dv['df_lda_sim'] = df_lda_sim\n",
    "    dv.map_sync(abs_similarity, nod_pair_list)\n",
    "    ##填充+归一化\n",
    "    df_lda_sim =  add_dv(dv['df_lda_sim'])\n",
    "    df_lda_sim = df_lda_sim.apply(lambda x: x/x.sum(),axis=1)  \n",
    "    print(subnet_name+'\\'s attribute abstract similarity has been finished')\n",
    "    return df_lda_sim\n",
    "\n",
    "#关键词\n",
    "def keywords_and_plus(paper_group, graph,nod_pair_list,w2v_model_path, subnet_name):\n",
    "    ##################################\n",
    "    def key_label(nod_label):\n",
    "        r1 = u'[a-zA-Z]'\n",
    "        index1 = nod_label[0]\n",
    "        candidate = nod_label[1]\n",
    "        global df_label,label_sum, w2v_model \n",
    "        for word in candidate:\n",
    "            word_loc = label_sum.index(word)\n",
    "            if word_loc<20:\n",
    "                word_loc = 20\n",
    "            for index2 in label_sum[word_loc-20:word_loc+20] :\n",
    "                if difflib.SequenceMatcher(None, index2, word).quick_ratio() > 0.98:\n",
    "                    df_label.loc[index1,index2] = 1        \n",
    "                elif 0.80<difflib.SequenceMatcher(None, index2, word).quick_ratio()<0.98:\n",
    "                    try:\n",
    "                        if len(re.sub(r1, '', index2)+re.sub(r1, '', word))== 0:\n",
    "                            if w2v_model.n_similarity(''.join(index2.split()),''.join(word.split())) >0.95:\n",
    "                                df_label.loc[index1,index2] = 1\n",
    "                        else:\n",
    "                            if w2v_model.n_similarity(''.join(convert_doc_to_wordlist(index2)),''.join(convert_doc_to_wordlist(word)) ) >0.93:\n",
    "                                df_label.loc[index1,index2] = 1\n",
    "                    except:\n",
    "                        pass\n",
    "        print(index1)\n",
    "    ##################################\n",
    "    def weight_word(keyword):\n",
    "        global word_weight,df_label\n",
    "        word_weight[keyword] = 1/(log(df_label.loc[:,keyword].sum(axis=0))+0.01)            \n",
    "    ##################################\n",
    "    def key_similarity(nod_pair):\n",
    "        index1 = nod_pair[0]\n",
    "        index2 = nod_pair[1]\n",
    "        global df_key_sim,df_lda_sim,word_weight          \n",
    "        paper_array = df_label.loc[index1,] + df_label.loc[index2,]\n",
    "        com_label = paper_array[paper_array==2]._stat_axis.values.tolist()\n",
    "        com_label_cou = len(com_label)\n",
    "        sim_sum = 0\n",
    "        if com_label_cou > 0:\n",
    "            for one_label in com_label:\n",
    "                sim_sum += word_weight[one_label]                    \n",
    "        df_key_sim[paper1, paper2] = sim_sum\n",
    "        df_key_sim[paper2, paper1] = sim_sum        \n",
    "    ##################################\n",
    "    print(subnet_name+'\\'s attribute keywords similarity starts to be constructed')                \n",
    "    #所有标签集合\n",
    "    label_sum = []\n",
    "    for key, item in paper_group.items():\n",
    "        label_sum.extend(item['ego_attribute']['keywords']+item['ego_attribute']['keyword_plus'])\n",
    "    label_sum = sorted(list(set(label_sum)))        #按首字母排序\n",
    "    ##构建二部网络矩阵\n",
    "    df_label = pd.DataFrame(index = graph.nodes(), columns = label_sum).fillna(0)         \n",
    "    ##判断词汇相似性利用w2v技术和字符串匹配,也就是相似度和距离\n",
    "    w2v_model = read_pkl(w2v_model_path)\n",
    "    ##构建nod_label_list\n",
    "    nod_label_list = []\n",
    "    for index1, item in paper_group.items():\n",
    "        candidate = list(set(item['ego_attribute']['keywords']+item['ego_attribute']['keyword_plus']))\n",
    "        nod_label_list.append([index1,candidate])\n",
    "    ##开启并行\n",
    "    dv = lan_para()\n",
    "    dv['df_label'] = df_label\n",
    "    dv['label_sum'] = label_sum\n",
    "    dv['w2v_model'] = w2v_model\n",
    "    dv.map_sync(key_label, nod_label_list)\n",
    "    df_label =  add_dv(dv['df_label'])\n",
    "    #单词计算权重，开启并行\n",
    "    dv = lan_para()\n",
    "    dv['df_label'] = df_label\n",
    "    dv['word_weight'] = {}\n",
    "    dv.map_sync(weight_word, label_sum)\n",
    "    word_weight = add_dv(dv['word_weight'])\n",
    "    #计算标签相似度\n",
    "    df_key_sim = pd.DataFrame(index = graph.nodes(), columns = graph.nodes()).fillna(0) \n",
    "    #开启并行\n",
    "    dv = lan_para()\n",
    "    dv['df_key_sim'] = df_key_sim\n",
    "    dv['df_label'] = df_label\n",
    "    dv['word_weight'] = word_weight\n",
    "    dv.map_sync(key_similarity, nod_pair_list)\n",
    "    df_key_sim = add_dv(dv['df_key_sim'])        \n",
    "    df_key_sim = df_key_sim.apply(lambda x: x/x.sum(),axis=1)       #归一化 \n",
    "    print(subnet_name+'\\'s attribute keywords similarity has been finished')                                \n",
    "    return df_key_sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相似度主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算论文节点相似度\n",
    "def paper_similarity(paper_cluster,path_dictionary, lda_model_path, w2v_model_path, path_df_prob_pkl, num_topics):\n",
    "    ##拆分名字\n",
    "    subnet_name = paper_cluster[0]\n",
    "    paper_group = paper_cluster[1] \n",
    "    ##构造有向图\n",
    "    print(subnet_name+'\\'s probability matrix'+'starts to be constructed')\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_nodes_from(list(paper_group.keys()))    #注意这里加入了所有点\n",
    "    paper_group_edge = combine_tuple(paper_group)     #不是所有点都具有连边\n",
    "    graph.add_edges_from(paper_group_edge)            #加入了部分节点的连边\n",
    "    ##构造nod_pair_list准备并行\n",
    "    nod_pair_list = []\n",
    "    paper_name_list = list(paper_group.keys())\n",
    "    paper_name_list_c = deepcopy(paper_name_list)\n",
    "    for index1 in paper_name_list:\n",
    "        for index2 in paper_name_list_c:\n",
    "            nod_pair_list.append([index1,index2])\n",
    "        paper_name_list_c.remove(index1)\n",
    "    ##构造拓扑相似度\n",
    "    print(subnet_name+'\\'s topology similarity'+'starts to be constructed')\n",
    "    df_topology = Topology_similarity(graph,subnet_name,nod_pair_list)          #拓扑相似度\n",
    "    print(subnet_name+'\\'s topology similarity'+'has been finished')\n",
    "    ##构造属性相似度\n",
    "    print(subnet_name+'\\'s attribute similarity'+'starts to be constructed')\n",
    "    df_attribute = Attribute_similarity(paper_group, graph, nod_pair_list, path_dictionary, lda_model_path, w2v_model_path, subnet_name,num_topics=4) #属性相似度\n",
    "    print(subnet_name+'\\'s attribute similarity'+'has been finished')\n",
    "    df_pro = 0.382*df_topology + 0.618*df_attribute   #加权\n",
    "    #归一化\n",
    "    df_prob = df_pro.apply(lambda x: x/x.sum(),axis=1)\n",
    "    path_df_prob_pkl_gen = path_df_prob_pkl + subnet_name + '.pkl'\n",
    "    df_prob_file = open(path_df_prob_pkl_gen, 'wb')\n",
    "    pkl.dump(df_prob, df_prob_file)\n",
    "    df_prob_file.close()\n",
    "    print(subnet_name+'\\'s probability matrix'+'has been finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "\n",
    "    path_paper_dict_pkl = '/data1/cufe/students/2017210761wsy/pkl/paper_dict.pkl'\n",
    "    paper_dict = read_pkl(path_paper_dict_pkl)\n",
    "    \n",
    "    ##制作路径\n",
    "    w2v_model_path = '/data1/cufe/students/2017210761wsy/model/w2v_model.pkl'\n",
    "    lda_model_path = '/data1/cufe/students/2017210761wsy/model/lda_true.pkl'\n",
    "    path_dictionary = '/data1/cufe/students/2017210761wsy/model/dictionary.pkl'\n",
    "    path_df_prob_pkl = '/data1/cufe/students/2017210761wsy/pkl/df_paper_prob_'\n",
    "    \n",
    "    ##切分子网络\n",
    "    paper_cluster = ('network',paper_dict)\n",
    "    #尝试运行paper_similarity\n",
    "    num_topics = 4\n",
    "    paper_similarity(paper_cluster,path_dictionary, lda_model_path, w2v_model_path, path_df_prob_pkl, num_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
